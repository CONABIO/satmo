# SATMO and the Mexican's ocean monitoring and analysis system

## Mexican marine ecosystems information and analysis system, general overview

SATMO is the satellite component of a complete marine monitoring and analysis system (*sistema de información y análisis de ecosistemas marino-costeros de México*). Another important component of the system will be the in-situ monitoring component. The in-situ system will ingest, analyze, and use both near real and a non-real time data (from buoys, stations, existing networks, university researchers, and crowd-sourcing). The data generated by the in-situ component of the system will play an important role in:

- Validating SATMO outputs
- Analysis aimed at answering research questions
- Generating alerts (algae bloom, coral bleaching, etc)

The overall system can be summarized with the conceptual diagram below.

![](img/SIMAR_overview.png)



Therefore *SATMO should be a point where analysis ready data from various satellite sources is available, facilitating efficient analysis with well integrated and up to date datasets.*

Direct applications of the overall system will be:

- Alert, forecast and early warning system for issues such as

    - Sargassum blooms (Detect and forecast future locations according to ocean currents)
    - Algae blooms (Including rapid alert to inform sanitary authorities that can subsequently test for toxicity)
    - Risks of coral bleaching

- Facilitated centralized access to various kind of data and information about seas and oceans


These activities should result in:

- An improved knowledge and understanding of marine ecosystems and dynamics
- A better capacity to monitor, protect, and manage marine ecosystems


## SATMO details

SATMO roles are to:

- Provide facilitated access to up to date information on various ocean parameters measurable by satellite

This implies that SATMO must

- Download relevant archived and upcoming satellite data in an operational manner
- Gather auxiliary products required for analysis and forecasting (e.g. Ocean currents)
- Compute ecologically meaningful and relevant variables
- Continuously produce and update information required to compute real time anomalies

A simplified overview of the SATMO processing chain can be found below. Further details can be found in the paragraphs below.

![](img/satmo_general_processing_chain.png)

## Detailed description of the ingestion and processing steps

### Data sources

This is a description of the fully automated ingestion and processing module of the SATMO. The main source of raw data is the ocean color [DAAC](http://oceandata.sci.gsfc.nasa.gov/). Processing from L0 to L1A is completely standard and performed automatically and in near real time by the ocean color system. L1A products are therefore preferred over L0 as they maintain all data characteristics and are suitable for re-calibrations, while limiting storage and processing requirements. Alternatively data can be obtained directly from the CONABIO antenna, as it receives, directly from various sensors, Production datasets (PDS). This option however, requires further data processing and is considered secondary at the moment and would only be used in case of dysfunction of the ocean color DAAC or in case of US government shutdown.

### Data download

#### Reflectance data (MODIS, SeaWifs, VIIRS)

- For near real time, data query and download can be facilited by the subscription service of the ocean color processing group. Subscriptions automatically select newly acquired files comprised in an area of interest and make them available on a share point. These files can then easily be retrieved.
- For downloading archive data, two python modules were developed to perform spatio-temporal queries and automate robust data download.


### Data processing

Processing is done automatically, triggered by the availability of the required data on the ocean color DAAC, with a flexible and modular design that combines existing tools with custom made solutions. The main pre-processing steps (e.g. generation of Level 3 mapped reflectance data (L3map_RRS)) are supported by the (already existing) SeaDAS processing modules, while later steps of the processing chain are implemented in python.
This flexible design enables transparency, the possibility to implement new algorithms in a timely manner as well as the capacity to accommodate new sensors as they are deployed.

The (daily, weekly and monthly) composites calculated are required to compute and update climatologies for each variables, from which anomalies can be generated.
More specifically, the data processing implemented in python will include:

1. Computation of daily ocean color and temperature indices (independently for each sensors)
2. Computation of combined products (combining daily indices (**1**) from all sensors)
3. Computation of weekly and monthly composites (from **2**)
4. Computation of (updated) daily, weekly, monthly climatologies for all variables (from **2** **3** and previous results of **4**)
5. Computation of daily (from **2** and **4**), weekly (from **3** and **4**) and monthly (from **3** and **4**) anomalies.

### SeaDAS processing (L1A to L3map)

We directly generate spatially binned and mapped reflectance data using the SeaDAS processing routines. This can be done in one step using the `multilevel-processor.py` 


### Calculation of indices

TODO: List of ocean color and temperature indices to be defined.

### Auxiliary data 

The generation of the spatial products mentioned above comes with auxiliary data. These auxiliary products are particularly useful for later data archiving, exploration and distribution. They comprise a minimum of:

- a jpg preview
- a json file with metadata (download url, preview url, cloud cover, processing date, variables, spatial characteristics, etc)

These additional output will be later used by the satmo data explorer (a dynamic website that generates product pages based on the content of the ftp site).

## System operation

The system must include both a near real time component -- capable of downloading data as they become available on the oceancolor servers and processing them immediately -- and a component that enables flexible updating or re-processing of different collections when needed.

The different modes are then:

- Near real time operation
- Updating mode (updates a given collection, only missing data are downloaded or processed)
- Re-process mode (reprocesses an entire collection)

### Administration interface

An interface to select variables, start/stop the system, update collections, etc... will be designed. Ideally with an authentication system connected to the conabio user database; otherwise using unix accounts of the server on which the system is running.

The interface will be written with the flask python web framework.

Controls of the administration interface will include:

- Start and stop NRT system
- Chose directories of input and output data
- Select variables to process
- Trigger a collection update or a re-processing

Additionally the interface should display:

- Status
- Errors and warnings

## User data access and exploration

### Long term goal

The ultimate goal of the SATMO + SIDMO projects combined is to enable visualization and analysis of products generated via a webmapping interface. The interface would enable users to visualize various products, perform simple analysis directly in the browser and order and download spatial and temporal subsets of the data. The visualization and analysis interface would be powered by an API. Advanced users could then also access the data non interactively by interacting directly with the API.

### First tier approach

Development of data access capabilities will follow a tiered approach. The first tier presented below enables a simple exploration and access of the data.
The data will be made available on a FTP server, which anyone can explore directly; however, to facilitate data browsing, we will set-up a dynamic website as well. Each product will have an html page generated by the dynamic website, including information on:

- Metadata
    - Variable
    - Cloud cover
    - Processing date
    - Aquisition date
    - Spatial attributes
- A jpeg preview
- Data download link
The figure below summarizes the architecture of this tier one solution.

![](img/data_exploration_first_tier.png)

We will develop the dynamic website using the python flask framework.

Although the conceptual design includes two servers (web and application), only a single machine is required. The data processing machine can possibly be used for that.

## Data volumes

The figures below are raw data (L1A) volume estimates till end of __2018__:

|     Sensor    | Yearly volume (GB) | Number of years | Total volume (GB) |
|---------------|--------------------|-----------------|-------------------|
| Seawifs (LAC) | 52                 |              14 | 728               |
| MODIS Terra   | 880                |              19 | 16,720            |
| MODIS Aqua    | 880                |              17 | 14,960            |
| VIIRS         | 2,140              |               7 | 14,980            |
| MERIS (FRS)   |                    |                 |                   |
| Sentinel 3a   |                    |                 |                   |
| Total         |                    |                 | 47,388            |
|               |                    |                 |                   |

These estimates include full sensor granules, which often include far more data than the the project area. Data volumes can probably be reduced by extracting L1A subsets, and archiving these only, though the gain of this approach is yet to be assessed.


